{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af6e12c-0e8f-433f-8b42-abe849152505",
   "metadata": {},
   "source": [
    "# Machine Learning using Python\n",
    "\n",
    "### Course Overview\n",
    "\n",
    "In this course we will learn how to apply machine learning using Python tools and focus on machine learning modeling techniques, such as **\"Classification\", \"Refression\", and \"Clustering\".**\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Describe how machine learning plays a pivot role in various career paths.\n",
    "2. Articulate the various stages involved in the machine learning lifecycle.\n",
    "3. Discuss how various machine learning models work.\n",
    "4. Implement machine learning models using Python and Scikit-learn.\n",
    "5. Solve data related problems using machine-learning models.\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "To get most of this course, we should be comfortable with the following topics and tehnologies:\n",
    "\n",
    "* Python and Python libraries such as pandas, numpy, and more to perform data preparation and data analysis.\n",
    "* High School Level mathematics is an asset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7200bc-6b90-4728-96bd-66e3e6503b09",
   "metadata": {},
   "source": [
    "### Course Outline:\n",
    "\n",
    "This course consits of 6 modules\n",
    "\n",
    "## Module 1: Introduction to machine Learning\n",
    "\n",
    "This module helps us with the knowledge of foundational machine learning concepts to delve deeper into applied machine learning modelling. We will learn that machine learning modelling is an interactive process with various lifecycle stages. We will also learn about the daily activities of a machine learning engineer. Here, we will be introduced to various open source tools for machine learning, including the popular Python package 'scikit-learn'.\n",
    "\n",
    "## Module 2: Linear and Logistic Regression\n",
    "\n",
    "This module will introduces us to two classical statistical methods, foundational to machine learning: Linear and Logistic Regression. We will learn how linear regression pioneered in the 1800s, models linear relationships while logistic regression serves as a classifier. Through implementing these models, We'll understand their limitations and gain insights into why modern machine learning models are offten preferred.\n",
    "\n",
    "## Module 3: Building supervised Learning models\n",
    "\n",
    "In this module, we will learn about implementing modern supervised machine learning models. W will start by understanding how binary classification works and discover how to constract a multiclass classifier from binary classification components. We will what desicion trees are, how they learn, and how to bulid them. Decision trees, which are used to solve classification problems, have a natural extension called 'regression trees', which can handel regression problems. We will learn about other supervised learning models, like KNN, and SVM. We will learn what bias and variance are in model fitting and the tradeoff between bias and variance that is inherent to all learning models in various degrees. We will learn for startegies for mitigating this trade off and work with models that do a very good job accomplishing that goal.\n",
    "\n",
    "## Module 4: Building Unsupervised Learning Models\n",
    "\n",
    "In this module we will dive into unsupervised learning, where algorithms uncover patterns in data without labeled examples. We will explore clustering startegies and real-world applications, focusing on techniques like hierarchical clustering, K-means, and advanced methods such as DBSCAN and HDBSCAN. Through practical labs, we will gain a deeper understanding of how to compare and implement these algorithms effectively. Adittionally, we wil delve into dimension reduction algorithms like PCA (Principal Component Analysis), t-SNE, and UMAP to reduce dataset features and simplify other modeling tasks. Using Python, we will implement these clustering and dimensionality reduction techniques, learning how to implement them with feature engineering to prepare data for machine learning models.\n",
    "\n",
    "## Module 5: Evaluating and Validating Machine Learning Models:\n",
    "\n",
    "This module covers how to assess model performance on unseen data, starting with key evaluation metrics for classification and regression. We will also explore hyperparameter tunining to optimize models while avoiding overfitting using cross-validation. Special techniques, such as regularization in linear regression, will be introduced to handle overfitting due to outliers. Hand-on excersies in python will guide us through model fitting and cross-validation for reliable model evaluation.\n",
    "\n",
    "## Module 6 - Final Project Exam and Project\n",
    "\n",
    "In this concluding module, we will review the course content, complete a final exam, and work on hands on project. We will recive a course summary cheat sheet, apply our skills in a project on Rain Prediction in Australia, and participate in peer reviews to share feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd74dbe-f6a8-45d4-aebc-d6834e4d4f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0a0ec-881f-41ae-b4b7-d2cb191d10c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "843a4675-73da-4b5b-b5a8-86975a7e93b2",
   "metadata": {},
   "source": [
    "# Module 1: Introduction to machine Learning\n",
    "\n",
    "Machine Learning is the subset of AI that uses algorithms and requires feature engineering by practioners. MAchine learning models learn using supervised, unsupervised, semi-supervised and reinforcement learning.\n",
    "\n",
    "Machine Learning consists of several techniques, namely\n",
    "1. Classification\n",
    "2. Regression\n",
    "3. Clustering\n",
    "4. Association\n",
    "5. Anomaly detection\n",
    "6. Sequence mining\n",
    "7. Dimension reduction\n",
    "8. Recommendation systems\n",
    "\n",
    "Machine learing is applied to predicted dieases, analyze consumer behaviour, recognize images and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3078b0-6e1a-4c65-a26d-95485c383a20",
   "metadata": {},
   "source": [
    "The processes involved in ML Model lifecycle are \n",
    "1. Problem defination\n",
    "2. Data Collection\n",
    "3. Data Preparation\n",
    "4. Model Development and Evaluation\n",
    "5. Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aaabb0-9820-45ed-bbc9-9d9a27ab5990",
   "metadata": {},
   "source": [
    "#### Tolls for Machine Learning\n",
    "\n",
    "* Data is central to an ML algorithm\n",
    "* ML tools simplify tasks and provide functionalities for ML pipelines\n",
    "* ML programming language is used to built ML models and decode data patterns.\n",
    "* Common languages: Python, R, Julia, Scala, Java, and JavaScript\n",
    "\n",
    "#### Tools\n",
    "\n",
    "* **Data Processing and analytics:** PostgreSQL, Hadoop, Spark, Apache Kafka, pandas and Numpy\n",
    "* **Data Visualization**: Matplotlib, Seaborn, ggplot2, and Tableau\n",
    "* **Machine Leaning**: Numpy, Pandas, SciPy, and scikit-learn\n",
    "* **Deep Learning**: TensorFlow, Keras, Theano, and Pytorch\n",
    "* **Computer vision**: OpenCV, scikit-image, and TorchVision\n",
    "* **NLP**: NLTK, TextBob, and Stanza\n",
    "* **Generative AI**: Hugging face transformers, ChatGPT, DALL-E, and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f4a5ad-3abb-43d0-b4d2-c6328d05e8bb",
   "metadata": {},
   "source": [
    "#### Scikit learn Machine learning Ecosystem \n",
    "\n",
    "Ml ecosystem refers to the interconnected tools, frameworks, libraries, platforms, and processes that support developing, deploying, and managing machine learning models\n",
    "\n",
    "It uses Python tools and libraries such as Numpy, Pandas, SciPy, Matplotlib, and Scikit-learn. Scikit-learn is a free machine learning library for Python that uses classification, regression, clustering and dimensionality reduction algorithms.\n",
    "\n",
    "Most tests required in a ML pipeline are already implemented in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ab29c-4ace-43d7-a7e9-c429e38b84b3",
   "metadata": {},
   "source": [
    "* Artificial intelligence (AI) simulates human cognition, while machine learning (ML) uses algorithms and requires feature engineering to learn from data.\n",
    "\n",
    "* Machine learning includes different types of models: supervised learning, which uses labeled data to make predictions; unsupervised learning, which finds patterns in unlabeled data; and semi-supervised learning, which trains on a small subset of labeled data.\n",
    "\n",
    "* Key factors for choosing a machine learning technique include the type of problem to be solved, the available data, available resources, and the desired outcome.\n",
    "\n",
    "* Machine learning techniques include anomaly detection for identifying unusual cases like fraud, classification for categorizing new data, regression for predicting continuous values, and clustering for grouping similar data points without labels.\n",
    "\n",
    "* Machine learning tools support pipelines with modules for data preprocessing, model building, evaluation, optimization, and deployment.\n",
    "\n",
    "* R is commonly used in machine learning for statistical analysis and data exploration, while Python offers a vast array of libraries for different machine learning tasks. Other programming languages used in ML include Julia, Scala, Java, and JavaScript, each suited to specific applications like high-performance computing and web-based ML models.\n",
    "\n",
    "* Data visualization tools such as Matplotlib and Seaborn create customizable plots, ggplot2 enables building graphics in layers, and Tableau provides interactive data dashboards.\n",
    "\n",
    "* Python libraries commonly used in machine learning include NumPy for numerical computations, Pandas for data analysis and preparation, SciPy for scientific computing, and Scikit-learn for building traditional machine learning models.\n",
    "\n",
    "* Deep learning frameworks such as TensorFlow, Keras, Theano, and PyTorch support the design, training, and testing of neural networks used in areas like computer vision and natural language processing.\n",
    "\n",
    "* Computer vision tools enable applications like object detection, image classification, and facial recognition, while natural language processing (NLP) tools like NLTK, TextBlob, and Stanza facilitate text processing, sentiment analysis, and language parsing.\n",
    "\n",
    "* Generative AI tools use artificial intelligence to create new content, including text, images, music, and other media, based on input data or prompts.\n",
    "\n",
    "* Scikit-learn provides a range of functions, including classification, regression, clustering, data preprocessing, model evaluation, and exporting models for production use.\n",
    "\n",
    "* The machine learning ecosystem includes a network of tools, frameworks, libraries, platforms, and processes that collectively support the development and management of machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3b11d-1e9f-47bb-ab1c-7e44ebee5b7b",
   "metadata": {},
   "source": [
    "# Module 2: Linear and Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a24ef2-5345-462a-87b4-6f08047930a7",
   "metadata": {},
   "source": [
    "* Regression models relationships between a continous target variable and explonatiry features, covering simple and multiple regression types.\n",
    "* Simple regressiojn uses a single independent variable to estimate a dependent variable, while multiple regression involves more than one independent variable.\n",
    "* Regression is widely applicable, from forecasting sales and estimating maintenace costs to predicting rainfall and diease spread.\n",
    "* In simple linear regression, a best fit line minimizes erros, measuered by mean squared error (MSE), this aproch is known as ordinary least squares(OLS).\n",
    "* OLS regression is easy to interpret but sensitive to outliers, which can impact accuracy.\n",
    "* Multiple linear regression extends simple linear regression by using multiple variables to predict outcomes and analyze variable relationships.\n",
    "* Adding too many variables can lead to overfitting, so careful variable selection is necessary to build a balanced model.\n",
    "* Non linear regression models complex relationships using polynomial, exponential, or logarithmic functions when data doesn't fit a straight line.\n",
    "* Polynomial regression can fit data but may overfit by capturing random noise rather than underlying patterns.\n",
    "* Logistic regression is a probability predictor and binary clasifier, suitable for binary targets and assesing feature impact.\n",
    "* Logistic regression minimizes errors using log-loss and optimizes with gradient descent or stochastic gradient descent for efficiency.\n",
    "* Gradient descent is an intractive process to minimize the cost function, which is crucial for training logistic regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72ea76-6d3a-4949-81b4-34f4e5627801",
   "metadata": {},
   "source": [
    "# Module 3: Building supervised Learning models\n",
    "\n",
    "* Classification is a supervised machine learning method used to predict labels on new data with applications in churn prediction, customer segmentation, loan default prediction, and multiclass drug prescriptions.\n",
    "\n",
    "* Binary classifiers can be extended to multiclass classification using one-versus-all or one-versus-one strategies.\n",
    "\n",
    "* A decision tree classifies data by testing features at each node, branching based on test results, and assigning classes at leaf nodes.\n",
    "\n",
    "* Decision tree training involves selecting features that best split the data and pruning the tree to avoid overfitting.\n",
    "\n",
    "* Information gain and Gini impurity are used to measure the quality of splits in decision trees.\n",
    "\n",
    "* Regression trees are similar to decision trees but predict continuous values by recursively splitting data to maximize information gain.\n",
    "\n",
    "* Mean Squared Error (MSE) is used to measure split quality in regression trees.\n",
    "\n",
    "* K-Nearest Neighbors (k-NN) is a supervised algorithm used for classification and regression by assigning labels based on the closest labeled data points.\n",
    "\n",
    "* To optimize k-NN, test various k values and measure accuracy, considering class distribution and feature relevance.\n",
    "\n",
    "* Support Vector Machines (SVM) build classifiers by finding a hyperplane that maximizes the margin between two classes, effective in high-dimensional spaces but sensitive to noise and large datasets.\n",
    "\n",
    "* The bias-variance tradeoff affects model accuracy, and methods such as bagging, boosting, and random forests help manage bias and variance to improve model performance.\n",
    "\n",
    "* Random forests use bagging to train multiple decision trees on bootstrapped data, improving accuracy by reducing variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c3ad0d-8939-4ab7-9fcd-7e2c6d32c118",
   "metadata": {},
   "source": [
    "## Module 4: Building Unsupervised Learning Models\n",
    "\n",
    "- Clustering is a machine learning technique used to group data based on similarity, with applications in customer segmentation and anomaly detection.\n",
    "\n",
    "- K-means clustering partitions data into clusters based on the distance between data points and centroids but struggles with imbalanced or non-convex clusters.\n",
    "\n",
    "- Heuristic methods such as silhouette analysis, the elbow method, and the Davies-Bouldin Index help assess k-means performance.\n",
    "\n",
    "- DBSCAN is a density-based algorithm that creates clusters based on density and works well with natural, irregular patterns.\n",
    "\n",
    "- HDBSCAN is a variant of DBSCAN that does not require parameters and uses cluster stability to find clusters.\n",
    "\n",
    "-  Hierarchical clustering can be divisive (top-down) or agglomerative (bottom-up) and produces a dendrogram to visualize the cluster hierarchy.\n",
    "\n",
    "- Dimension reduction simplifies data structure, improves clustering outcomes, and is useful in tasks such as face recognition (using eigenfaces).\n",
    "\n",
    "- Clustering and dimension reduction work together to improve model performance by reducing noise and simplifying feature selection.\n",
    "\n",
    "- PCA, a linear dimensionality reduction method, minimizes information loss while reducing dimensionality and noise in data.\n",
    "\n",
    "- t-SNE and UMAP are other dimensionality reduction techniques that map high-dimensional data into lower-dimensional spaces for visualization and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bb470-3e18-4ef7-93b0-8a9fdfd3c3dd",
   "metadata": {},
   "source": [
    "## Module 5: Evaluating and Validating Machine Learning Models:\n",
    "\n",
    "- Supervised learning evaluation assesses a model's ability to predict outcomes for unseen data, often using a train/test split to estimate performance.\n",
    "\n",
    "- Key metrics for classification evaluation include accuracy, confusion matrix, precision, recall, and the F1 score, which balances precision and recall.\n",
    "\n",
    "- Regression model evaluation metrics include MAE, MSE, RMSE, R-squared, and explained variance to measure prediction accuracy.\n",
    "\n",
    "- Unsupervised learning models are evaluated for pattern quality and consistency using metrics like Silhouette Score, Davies-Bouldin Index, and Adjusted Rand Index.\n",
    "\n",
    "- Dimensionality reduction evaluation involves Explained Variance Ratio, Reconstruction Error, and Neighborhood Preservation to assess data structure retention.\n",
    "\n",
    "- Model validation, including dividing data into training, validation, and test sets, helps prevent overfitting by tuning hyperparameters carefully.\n",
    "\n",
    "- Cross-validation methods, especially K-fold and stratified cross-validation, support robust model validation without overfitting to test data.\n",
    "\n",
    "- Regularization techniques, such as ridge (L2) and lasso (L1) regression, help prevent overfitting by adding penalty terms to linear regression models.\n",
    "\n",
    "- Data leakage occurs when training data includes information unavailable in real-world data, which is preventable by separating data properly and mindful feature selection.\n",
    "\n",
    "- Common modeling pitfalls include misinterpreting feature importance, ignoring class imbalance, and relying excessively on automated processes without causal analysis.\n",
    "\n",
    "- Feature importance assessments should consider redundancy, scale sensitivity, and avoid misinterpretation, as well as inappropriate assumptions about causation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aedf67-1797-483a-930c-b95699ca2630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
